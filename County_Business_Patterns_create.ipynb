{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import DB_Utilities\n",
    "DBTools = DB_Utilities.DBTools()  # instantiate the class\n",
    "from  File_Utilities import FileTools\n",
    "FileTools.MYDIR = \".\\data\"\n",
    "\n",
    "redownload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_year(year):\n",
    "    if redownload:\n",
    "        print(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}co.zip\")\n",
    "        FileTools.unzip_file(FileTools.get_file_from_url(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}co.zip\"))\n",
    "\n",
    "        print(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}st.zip\")\n",
    "        FileTools.unzip_file(FileTools.get_file_from_url(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}st.zip\"))\n",
    "\n",
    "        print(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}us.zip\")\n",
    "        FileTools.unzip_file(FileTools.get_file_from_url(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}us.zip\"))\n",
    "\n",
    "    # df_county = pd.read_csv(FileTools.get_full_file_path(f'cbp{year[2:]}co.txt'))\n",
    "    df_county = FileTools.load_df_from_csv(f'cbp{year[2:]}co.txt')\n",
    "    # df_state = pd.read_csv(FileTools.get_full_file_path(f'cbp{year[2:]}st.txt'))\n",
    "    df_state = FileTools.load_df_from_csv(f'cbp{year[2:]}st.txt')\n",
    "    # df_us = pd.read_csv(FileTools.get_full_file_path(f'cbp{year[2:]}us.txt'))\n",
    "    df_us = FileTools.load_df_from_csv(f'cbp{year[2:]}us.txt')\n",
    "\n",
    "    return df_county, df_state, df_us\n",
    "\n",
    "# test\n",
    "# df_county, df_state, df_us = get_file_by_year('2020')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_names_toLowerCase(df):\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    return df\n",
    "\n",
    "def munge_data(_df):\n",
    "\n",
    "    # they changed the case of the column names in 2016. Jerks.\n",
    "    _df = convert_column_names_toLowerCase(_df)\n",
    "    \n",
    "\n",
    "    _df['naics'] = _df['naics'].str.replace('-', '')\n",
    "    _df['naics'] = _df['naics'].str.replace(' ', '')\n",
    "    _df['naics'] = _df['naics'].str.replace('/', '')\n",
    "    _df['naics_level'] = _df['naics'].str.len()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # If there is no county_fips just don't do this. \n",
    "        # This means that this data is coming from either the state or the us.\n",
    "            # Get rid of counties with 999 fips, they are either statewide or unknown.\n",
    "        _df = _df[_df['fipscty'] != 999].copy()\n",
    "        _df['county_fips'] = _df['fipstate'].astype(str).str.zfill(2)+_df['fipscty'].astype(str).str.zfill(3)    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass # hahaha. not handling this error today.\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(year=\"2020\", force_run=False):\n",
    "   print(f\"Running for year {year}\")\n",
    "\n",
    "   file_name = FileTools.get_full_file_path(f'cbp_emp_percent_by_county_state_us_{year}.gzip')   \n",
    "   does_file_exist = FileTools.check_file(file_name)\n",
    "   # if not then make it...\n",
    "   if does_file_exist and not force_run:              \n",
    "      print(f\"Loading from file : {file_name} .\")\n",
    "      # emp_percent_by_county_state_us = pd.read_csv(file_name)\n",
    "      emp_percent_by_county_state_us = FileTools.load_df_from_parquet(f'cbp_emp_percent_by_county_state_us_{year}.gzip')\n",
    "   else:\n",
    "      print(f\"Making from scratch for : {year}\")\n",
    "      # Do all the work and save it.\n",
    "      df_county, df_state, df_us =  get_file_by_year(year)\n",
    "\n",
    "      print('Munging data')\n",
    "      df_county = munge_data(df_county)\n",
    "      # We only want top level employmnet data at the state and us level.\n",
    "      # LFO             C       Legal Form of Organization\n",
    "\n",
    "      #                         '-' - All Establishments                        \n",
    "      #                         C - C-Corporations and other corporate legal forms of organization\n",
    "      #                         Z - S-Corporations\n",
    "      #                         S - Sole Proprietorships\n",
    "      #                         P - Partnerships\n",
    "      #                         N - Non-Profits\n",
    "      #                         G - Government\n",
    "      #                         O - Other\n",
    "\n",
    "      df_state = munge_data(df_state)\n",
    "      df_state = df_state[df_state.lfo == '-'] #Only All Establishments\n",
    "      df_us = munge_data(df_us)\n",
    "      df_us = df_us[df_us.lfo == '-'] #Only All Establishments\n",
    "\n",
    "\n",
    "      print('Getting region level employment')\n",
    "      # county level employment, we will join on this later as our base data\n",
    "      df_county_emp = df_county[df_county.naics_level == 0][['county_fips','emp']]\n",
    "      # state level employment, we need this to calculate the locaton quotient in relation to the state.\n",
    "      df_state_emp = df_state[(df_state.naics_level == 0) ][['fipstate','emp']]\n",
    "      # us level employment, we need this to calculate the locaton quotient in relation to the us.\n",
    "      df_us_emp = df_us[(df_us.naics_level == 0) ][['uscode','emp']]\n",
    "\n",
    "\n",
    "      print('Getting county level employment by industry')\n",
    "      # Get the percent of each naics code in each county\n",
    "      emp_percent_by_county = pd.merge(df_county, df_county_emp, on=\"county_fips\")\n",
    "      emp_percent_by_county['percent_of_county_emp'] = emp_percent_by_county['emp_x']/emp_percent_by_county['emp_y']\n",
    "      emp_percent_by_county.rename(columns = {'emp_x':'emp_county_naics' , 'emp_y':'emp_county'}, inplace = True)\n",
    "\n",
    "      print('Getting state level employment by industry')\n",
    "      # Get the percent of each naics code in each state\n",
    "      emp_percent_by_state = pd.merge(df_state, df_state_emp, on=\"fipstate\")\n",
    "      emp_percent_by_state['percent_of_state_emp'] = emp_percent_by_state['emp_x']/emp_percent_by_state['emp_y']\n",
    "      emp_percent_by_state.rename(columns = {'emp_x':'emp_state_naics', 'emp_y':'emp_state' }, inplace = True)\n",
    "      emp_percent_by_state = emp_percent_by_state[['fipstate','naics','percent_of_state_emp', 'emp_state_naics', 'emp_state']]\n",
    "\n",
    "      print('Getting us level employment by industry')\n",
    "      # Get the percent of each naics code in the US\n",
    "      emp_percent_by_us = pd.merge(df_us, df_us_emp, on=\"uscode\")\n",
    "      emp_percent_by_us['percent_of_us_emp'] = emp_percent_by_us['emp_x']/emp_percent_by_us['emp_y']\n",
    "      emp_percent_by_us.rename(columns = {'emp_x':'emp_us_naics', 'emp_y':'emp_us' }, inplace = True)\n",
    "      emp_percent_by_us = emp_percent_by_us[['naics','percent_of_us_emp', 'emp_us_naics', 'emp_us']]\n",
    "      emp_percent_by_us.head()\n",
    "\n",
    "      print('Merging county with state and us')\n",
    "      # Merge all the data together so we can calculate the location quotient.\n",
    "      emp_percent_by_county_state = pd.merge(emp_percent_by_county, emp_percent_by_state, how='left', left_on=[\"naics\", 'fipstate'], right_on=[\"naics\", 'fipstate'])\n",
    "      emp_percent_by_county_state_us = pd.merge(emp_percent_by_county_state, emp_percent_by_us, how='left', left_on=[\"naics\"], right_on=[\"naics\"])\n",
    "\n",
    "      print('Calculating location quotient')\n",
    "      # Calculate the location quotient for county/state and for county/us\n",
    "      emp_percent_by_county_state_us['location_quotient_county_state'] = emp_percent_by_county_state_us['percent_of_county_emp']/emp_percent_by_county_state_us['percent_of_state_emp']\n",
    "      emp_percent_by_county_state_us['location_quotient_county_us'] = emp_percent_by_county_state_us['percent_of_county_emp']/emp_percent_by_county_state_us['percent_of_us_emp']\n",
    "      \n",
    "      emp_percent_by_county_state_us.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "      emp_percent_by_county_state_us['year'] = year\n",
    "#\n",
    "      print('Saving to file')\n",
    "      FileTools.save_df_as_parquet(emp_percent_by_county_state_us, f'cbp_emp_percent_by_county_state_us_{year}.gzip')\n",
    "      print('Writing to dababase')\n",
    "      # DBTools.truncate_and_insert_df(emp_percent_by_county_state_us, f\"cbp_emp_percent_by_county_state_us_{year}\")\n",
    "      # DBTools.insert_df(emp_percent_by_county_state_us, f\"cbp_emp_percent_by_county_state_us_{year}\")\n",
    "\n",
    "\n",
    "   return emp_percent_by_county_state_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_get = [2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]\n",
    "# years_to_get = [2020]\n",
    "Force_Run = False\n",
    "\n",
    "dataframes = []\n",
    "for year in years_to_get:\n",
    "    dataframes.append(run_all(str(year), Force_Run))\n",
    "\n",
    "all_years  = FileTools.concatenate_dataframes(dataframes)\n",
    "all_years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naics_level = 2\n",
    "# FileTools.save_df_as_parquet(all_years, f\"cbp_lq_all_years.gzip\")\n",
    "all_years_naics  = all_years[all_years.naics_level == naics_level].copy()\n",
    "\n",
    "# parquet is particular about mixed types, so we need to convert the objects to strings.\n",
    "for df_column in all_years_naics.select_dtypes(include=['object']).columns:    \n",
    "    all_years_naics [df_column] = all_years_naics [df_column].astype(str)\n",
    "\n",
    "FileTools.save_df_as_parquet(all_years_naics , f\"cbp_lq_{naics_level}digit_naics_all_years.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting tables so county is row and all predictors are columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data.\n",
    "_cbp_lq_digit_naics_all_years_df = FileTools.load_df_from_parquet(f\"cbp_lq_{naics_level}digit_naics_all_years.gzip\")                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cbp = pd.DataFrame()\n",
    "for i in range(2010,2021): \n",
    "# for i in range(2020,2021): \n",
    "    year = str(i)\n",
    "    print(f'Working on year : {year}')    \n",
    "\n",
    "    # Filter by the year we want.\n",
    "    temp_cbp_lq_digit_naics_all_years_df = _cbp_lq_digit_naics_all_years_df[\n",
    "        (_cbp_lq_digit_naics_all_years_df['year'] == year) \n",
    "        & (_cbp_lq_digit_naics_all_years_df['naics_level'] == naics_level)\n",
    "        ][[\n",
    "            'county_fips','qp1', 'ap', 'est','naics','location_quotient_county_state','location_quotient_county_us']].copy()\n",
    "\n",
    "\n",
    "    # Get some aggregations by county\n",
    "    group_cbp_lq_digit_naics_all_years_df = temp_cbp_lq_digit_naics_all_years_df[['county_fips','qp1', 'ap', 'est']].groupby(['county_fips']).sum().reset_index()\n",
    "    \n",
    "    \n",
    "    # Pivot all the naics codes up into columns\n",
    "    pivot_cbp_lq_2digit_naics_all_years_df = pd.pivot_table(temp_cbp_lq_digit_naics_all_years_df, values='location_quotient_county_us', index=['county_fips'],\n",
    "                        columns=['naics'], aggfunc=np.average, fill_value=0)\n",
    "\n",
    "\n",
    "    # Add the aggregations to the pivot table\n",
    "    merge_cbp_lq_2digit_naics_all_years_df = pivot_cbp_lq_2digit_naics_all_years_df.merge(group_cbp_lq_digit_naics_all_years_df, on='county_fips', how='left')\n",
    "    merge_cbp_lq_2digit_naics_all_years_df['year'] = year\n",
    "\n",
    "    # Do I need to do this?\n",
    "    merge_cbp_lq_2digit_naics_all_years_df.reset_index(inplace=True)\n",
    "    all_data_cbp = pd.concat([all_data_cbp, merge_cbp_lq_2digit_naics_all_years_df])\n",
    "\n",
    "\n",
    "# Save the data to a parquet file.\n",
    "FileTools.save_df_as_parquet(all_data_cbp, f'cbp_lq_{naics_level}digit_naics_all_years_PIVOT.gzip')\n",
    "FileTools.save_df_as_csv(all_data_cbp, f'cbp_lq_{naics_level}digit_naics_all_years_PIVOT.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For two digit data\n",
    "\n",
    "if(naics_level == 2):\n",
    "\n",
    "    rename_dict = {'11':'Agriculture_Forestry_Fishing_Hunting',\n",
    "    '21':'Mining_Quarrying_and_Oil_Gas_Extraction',\n",
    "    '22':'Utilities',\n",
    "    '23':'Construction',\n",
    "    '31':'Manufacturing',\n",
    "    '42':'Wholesale_Trade',\n",
    "    '44':'Retail_Trade',\n",
    "    '48':'Transportation_Warehousing',\n",
    "    '51':'Information',\n",
    "    '52':'Finance_Insurance',\n",
    "    '53':'Real_Estate_Rental_Leasing',\n",
    "    '54':'Professional_Scientific_and_Technical_Services',\n",
    "    '55':'Management_of_Companies_Enterprises',\n",
    "    '56':'Administrative_Support_Waste_Management_Remediation_Services',\n",
    "    '61':'Educational_Services',\n",
    "    '62':'Health_Care_Social_Assistance',\n",
    "    '71':'Arts_Entertainment_and_Recreation',\n",
    "    '72':'Accommodation_Food_Services',\n",
    "    '81':'Other_Services_except_Public_Administration',\n",
    "    '92':'Public_Administration'}\n",
    "\n",
    "    all_data_cbp.rename(columns=rename_dict, inplace=True)  \n",
    "\n",
    "\n",
    "    for i, k in enumerate(rename_dict):\n",
    "        print(i, k)\n",
    "\n",
    "\n",
    "\n",
    "    for i, k in enumerate(rename_dict):\n",
    "        print(rename_dict[k])\n",
    "        column_name = rename_dict[k]  \n",
    "        try:  \n",
    "            all_data_cbp[f'{column_name}_base'] = np.where(all_data_cbp[column_name] > 1, 1, 0)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    FileTools.save_df_as_parquet(all_data_cbp, f'cbp_lq_{naics_level}digit_naics_all_years_base_PIVOT.gzip')\n",
    "    FileTools.save_df_as_csv(all_data_cbp, f'cbp_lq_{naics_level}digit_naics_all_years_base_PIVOT.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_data_cbp = FileTools.load_df_from_parquet(f'cbp_lq_{naics_level}digit_naics_all_years_base_PIVOT.gzip')\n",
    "print(test_all_data_cbp.shape)\n",
    "\n",
    "# DBTools.truncate_and_insert_df(all_data_cbp, \"cbp_PIVOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_data_cbp.query(\"year == '2020'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the data with patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_patents = pd.read_csv('./data/patentsview/all_patents_granted_and pregrant.csv',dtype={'GEOID':'string', 'grant_year':'Int64', 'application_year':'Int64'}, low_memory=False)\n",
    "_patents.GEOID = _patents.GEOID.str.rjust(5, '0')\n",
    "\n",
    "_cbp_2_digit = pd.read_parquet(f'./data/cbp/cbp_lq_2digit_naics_all_years_base_PIVOT.gzip')\n",
    "_cbp_2_digit = _cbp_2_digit.astype({'year': 'Int64'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GEOID               string\n",
       "application_year     Int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_patents[['GEOID', 'application_year']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "county_fips    object\n",
       "year            Int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cbp_2_digit[['county_fips', 'year']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n"
     ]
    }
   ],
   "source": [
    "# Merge bea gdp and patents data\n",
    "test_merge = pd.merge(_patents, _cbp_2_digit,\n",
    "                      left_on = ['GEOID', 'application_year'],\n",
    "                      right_on = ['county_fips', 'year'],\n",
    "                      how = 'outer',\n",
    "                      indicator = 'matched',\n",
    "                      validate = 'many_to_one')\n",
    "\n",
    "print(len(test_merge.query(\"matched == 'left_only'\").GEOID.unique()))\n",
    "# The missing values\n",
    "missing_values_patents = pd.DataFrame(test_merge.query(\"matched == 'left_only'\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['35028', '42057', '32029', '46021', '46102', '47127', '78030',\n",
       "       '72127', '66010', '69110', '72025', '72053', '72021', '51075',\n",
       "       '72091', '72001', '72023', '72103', '72077', '72061', '72031',\n",
       "       '78010', '72069', '13289', '20031', '72113', '72097', '72133',\n",
       "       '72153', '72139', '72027', '72117'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_merge_no_terr = test_merge[~test_merge.GEOID.str.startswith(('78', '72' , '69','66' ))]\n",
    "print(len(test_merge.query(\"matched == 'left_only' & application_year > 2009\", engine='python').GEOID.unique()))\n",
    "test_merge.query(\"matched == 'left_only' & application_year > 2009\", engine='python').GEOID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('UVACapstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1320d6b9c24963eeab91ed3c1b469bbfc0dc38b81cdaee0abc39ad090f12f690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
