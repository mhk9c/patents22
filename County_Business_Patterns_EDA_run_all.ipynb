{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import DB_Utilities\n",
    "DBTools = DB_Utilities.DBTools()  # instantiate the class\n",
    "from  File_Utilities import FileTools\n",
    "FileTools.MYDIR = \".\\data\"\n",
    "\n",
    "redownload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_by_year(year):\n",
    "    if redownload:\n",
    "        print(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}co.zip\")\n",
    "        FileTools.unzip_file(FileTools.get_file_from_url(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}co.zip\"))\n",
    "\n",
    "        print(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}st.zip\")\n",
    "        FileTools.unzip_file(FileTools.get_file_from_url(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}st.zip\"))\n",
    "\n",
    "        print(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}us.zip\")\n",
    "        FileTools.unzip_file(FileTools.get_file_from_url(f\"https://www2.census.gov/programs-surveys/cbp/datasets/{year}/cbp{year[2:]}us.zip\"))\n",
    "\n",
    "    # df_county = pd.read_csv(FileTools.get_full_file_path(f'cbp{year[2:]}co.txt'))\n",
    "    df_county = FileTools.load_df_from_csv(f'cbp{year[2:]}co.txt')\n",
    "    # df_state = pd.read_csv(FileTools.get_full_file_path(f'cbp{year[2:]}st.txt'))\n",
    "    df_state = FileTools.load_df_from_csv(f'cbp{year[2:]}st.txt')\n",
    "    # df_us = pd.read_csv(FileTools.get_full_file_path(f'cbp{year[2:]}us.txt'))\n",
    "    df_us = FileTools.load_df_from_csv(f'cbp{year[2:]}us.txt')\n",
    "\n",
    "    return df_county, df_state, df_us\n",
    "\n",
    "# test\n",
    "# df_county, df_state, df_us = get_file_by_year('2020')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_names_toLowerCase(df):\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    return df\n",
    "\n",
    "def munge_data(_df):\n",
    "\n",
    "    # they changed the case of the column names in 2016. Jerks.\n",
    "    _df = convert_column_names_toLowerCase(_df)\n",
    "    \n",
    "\n",
    "    _df['naics'] = _df['naics'].str.replace('-', '')\n",
    "    _df['naics'] = _df['naics'].str.replace(' ', '')\n",
    "    _df['naics'] = _df['naics'].str.replace('/', '')\n",
    "    _df['naics_level'] = _df['naics'].str.len()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # If there is no county_fips just don't do this. \n",
    "        # This means that this data is coming from either the state or the us.\n",
    "            # Get rid of counties with 999 fips, they are either statewide or unknown.\n",
    "        _df = _df[_df['fipscty'] != 999].copy()\n",
    "        _df['county_fips'] = _df['fipstate'].astype(str).str.zfill(2)+_df['fipscty'].astype(str).str.zfill(3)    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass # hahaha. not handling this error today.\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(year=\"2020\", force_run=False):\n",
    "   print(f\"Running for year {year}\")\n",
    "\n",
    "   file_name = FileTools.get_full_file_path(f'cbp_emp_percent_by_county_state_us_{year}.gzip')   \n",
    "   does_file_exist = FileTools.check_file(file_name)\n",
    "   # if not then make it...\n",
    "   if does_file_exist and not force_run:              \n",
    "      print(f\"Loading from file : {file_name} .\")\n",
    "      # emp_percent_by_county_state_us = pd.read_csv(file_name)\n",
    "      emp_percent_by_county_state_us = FileTools.load_df_from_parquet(f'cbp_emp_percent_by_county_state_us_{year}.gzip')\n",
    "   else:\n",
    "      print(f\"Making from scratch for : {year}\")\n",
    "      # Do all the work and save it.\n",
    "      df_county, df_state, df_us =  get_file_by_year(year)\n",
    "\n",
    "      print('Munging data')\n",
    "      df_county = munge_data(df_county)\n",
    "      # We only want top level employmnet data at the state and us level.\n",
    "      # LFO             C       Legal Form of Organization\n",
    "\n",
    "      #                         '-' - All Establishments                        \n",
    "      #                         C - C-Corporations and other corporate legal forms of organization\n",
    "      #                         Z - S-Corporations\n",
    "      #                         S - Sole Proprietorships\n",
    "      #                         P - Partnerships\n",
    "      #                         N - Non-Profits\n",
    "      #                         G - Government\n",
    "      #                         O - Other\n",
    "\n",
    "      df_state = munge_data(df_state)\n",
    "      df_state = df_state[df_state.lfo == '-'] #Only All Establishments\n",
    "      df_us = munge_data(df_us)\n",
    "      df_us = df_us[df_us.lfo == '-'] #Only All Establishments\n",
    "\n",
    "\n",
    "      print('Getting region level employment')\n",
    "      # county level employment, we will join on this later as our base data\n",
    "      df_county_emp = df_county[df_county.naics_level == 0][['county_fips','emp']]\n",
    "      # state level employment, we need this to calculate the locaton quotient in relation to the state.\n",
    "      df_state_emp = df_state[(df_state.naics_level == 0) ][['fipstate','emp']]\n",
    "      # us level employment, we need this to calculate the locaton quotient in relation to the us.\n",
    "      df_us_emp = df_us[(df_us.naics_level == 0) ][['uscode','emp']]\n",
    "\n",
    "\n",
    "      print('Getting county level employment by industry')\n",
    "      # Get the percent of each naics code in each county\n",
    "      emp_percent_by_county = pd.merge(df_county, df_county_emp, on=\"county_fips\")\n",
    "      emp_percent_by_county['percent_of_county_emp'] = emp_percent_by_county['emp_x']/emp_percent_by_county['emp_y']\n",
    "      emp_percent_by_county.rename(columns = {'emp_x':'emp_county_naics' , 'emp_y':'emp_county'}, inplace = True)\n",
    "\n",
    "      print('Getting state level employment by industry')\n",
    "      # Get the percent of each naics code in each state\n",
    "      emp_percent_by_state = pd.merge(df_state, df_state_emp, on=\"fipstate\")\n",
    "      emp_percent_by_state['percent_of_state_emp'] = emp_percent_by_state['emp_x']/emp_percent_by_state['emp_y']\n",
    "      emp_percent_by_state.rename(columns = {'emp_x':'emp_state_naics', 'emp_y':'emp_state' }, inplace = True)\n",
    "      emp_percent_by_state = emp_percent_by_state[['fipstate','naics','percent_of_state_emp', 'emp_state_naics', 'emp_state']]\n",
    "\n",
    "      print('Getting us level employment by industry')\n",
    "      # Get the percent of each naics code in the US\n",
    "      emp_percent_by_us = pd.merge(df_us, df_us_emp, on=\"uscode\")\n",
    "      emp_percent_by_us['percent_of_us_emp'] = emp_percent_by_us['emp_x']/emp_percent_by_us['emp_y']\n",
    "      emp_percent_by_us.rename(columns = {'emp_x':'emp_us_naics', 'emp_y':'emp_us' }, inplace = True)\n",
    "      emp_percent_by_us = emp_percent_by_us[['naics','percent_of_us_emp', 'emp_us_naics', 'emp_us']]\n",
    "      emp_percent_by_us.head()\n",
    "\n",
    "      print('Merging county with state and us')\n",
    "      # Merge all the data together so we can calculate the location quotient.\n",
    "      emp_percent_by_county_state = pd.merge(emp_percent_by_county, emp_percent_by_state, how='left', left_on=[\"naics\", 'fipstate'], right_on=[\"naics\", 'fipstate'])\n",
    "      emp_percent_by_county_state_us = pd.merge(emp_percent_by_county_state, emp_percent_by_us, how='left', left_on=[\"naics\"], right_on=[\"naics\"])\n",
    "\n",
    "      print('Calculating location quotient')\n",
    "      # Calculate the location quotient for county/state and for county/us\n",
    "      emp_percent_by_county_state_us['location_quotient_county_state'] = emp_percent_by_county_state_us['percent_of_county_emp']/emp_percent_by_county_state_us['percent_of_state_emp']\n",
    "      emp_percent_by_county_state_us['location_quotient_county_us'] = emp_percent_by_county_state_us['percent_of_county_emp']/emp_percent_by_county_state_us['percent_of_us_emp']\n",
    "      \n",
    "      emp_percent_by_county_state_us.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "      emp_percent_by_county_state_us['year'] = year\n",
    "\n",
    "      FileTools.save_df_as_parquet(emp_percent_by_county_state_us, f'cbp_emp_percent_by_county_state_us_{year}.gzip')\n",
    "      # DBTools.truncate_and_insert_df(emp_percent_by_county_state_us, f\"cbp_emp_percent_by_county_state_us_{year}\")\n",
    "\n",
    "\n",
    "   return emp_percent_by_county_state_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for year 2020\n",
      "Loading from file : .\\data\\cbp_emp_percent_by_county_state_us_2020.gzip .\n",
      "Running for year 2019\n",
      "Making from scratch for : 2019\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2018\n",
      "Making from scratch for : 2018\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2017\n",
      "Making from scratch for : 2017\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2016\n",
      "Making from scratch for : 2016\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2015\n",
      "Making from scratch for : 2015\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2014\n",
      "Making from scratch for : 2014\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2013\n",
      "Making from scratch for : 2013\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2012\n",
      "Making from scratch for : 2012\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2011\n",
      "Making from scratch for : 2011\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n",
      "Running for year 2010\n",
      "Making from scratch for : 2010\n",
      "Munging data\n",
      "'fipscty'\n",
      "'fipscty'\n",
      "Getting region level employment\n",
      "Getting county level employment by industry\n",
      "Getting state level employment by industry\n",
      "Getting us level employment by industry\n",
      "Merging county with state and us\n",
      "Calculating location quotient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19140419, 40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_to_get = [2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010]\n",
    "# years_to_get = [2020]\n",
    "\n",
    "dataframes = []\n",
    "for year in years_to_get:\n",
    "    dataframes.append(run_all(str(year), False))\n",
    "\n",
    "all_years  = FileTools.concatenate_dataframes(dataframes)\n",
    "all_years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naics\n",
      "emp_nf\n",
      "qp1_nf\n",
      "ap_nf\n",
      "n<5\n",
      "n5_9\n",
      "n10_19\n",
      "n20_49\n",
      "n50_99\n",
      "n100_249\n",
      "n250_499\n",
      "n500_999\n",
      "n1000\n",
      "n1000_1\n",
      "n1000_2\n",
      "n1000_3\n",
      "n1000_4\n",
      "county_fips\n",
      "year\n",
      "empflag\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cbp_lq_2digit_naics_all_years.gzip'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FileTools.save_df_as_parquet(all_years, f\"cbp_lq_all_years.gzip\")\n",
    "all_years_2digit_naics = all_years[all_years.naics_level == 2].copy()\n",
    "\n",
    "# parquet is particular about mixed types, so we need to convert the objects to strings.\n",
    "for df_column in all_years_2digit_naics.select_dtypes(include=['object']).columns:    \n",
    "    all_years_2digit_naics[df_column] = all_years_2digit_naics[df_column].astype(str)\n",
    "\n",
    "FileTools.save_df_as_parquet(all_years_2digit_naics, f\"cbp_lq_2digit_naics_all_years.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naics\n",
      "emp_nf\n",
      "qp1_nf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_1860\\1250420478.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_years_2digit_naics[df_column] = all_years_2digit_naics[df_column].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap_nf\n",
      "n<5\n",
      "n5_9\n",
      "n10_19\n",
      "n20_49\n",
      "n50_99\n",
      "n100_249\n",
      "n250_499\n",
      "n500_999\n",
      "n1000\n",
      "n1000_1\n",
      "n1000_2\n",
      "n1000_3\n",
      "n1000_4\n",
      "county_fips\n",
      "year\n",
      "empflag\n"
     ]
    }
   ],
   "source": [
    "# all_years_2digit_naics.dtypes\n",
    "for df_column in all_years_2digit_naics.select_dtypes(include=['object']).columns:\n",
    "    print(df_column)\n",
    "    all_years_2digit_naics[df_column] = all_years_2digit_naics[df_column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2739]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('UVACapstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1320d6b9c24963eeab91ed3c1b469bbfc0dc38b81cdaee0abc39ad090f12f690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
