{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General purpose data cleaning. \n",
    "#### If you have cleaned a file let's keep track of what we did here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geocoder\n",
    "import requests\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "import json \n",
    "from sqlalchemy.sql import text\n",
    "import boto3\n",
    "\n",
    "from File_Utilities import FileTools\n",
    "import DB_Utilities\n",
    "DBTools = DB_Utilities.DBTools()  # instantiate the class\n",
    "FileTools.MYDIR = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making crosswalk for: merging on city name and state name to fips codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write some methods to use a geocoder to get the results and examine them in a more systematic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('location')\n",
    "geolocator = Nominatim(user_agent=\"UVaPatent22Capstone_DataScience\")\n",
    "null_response = {'original_state' : \"\",'original_city' : \"\",'county_GEOID' : \"\",'county_fip' :\"\", 'county_BASENAME' :\"\",'state_fip' :\"\",'state_BASENAME' : \"\",'state_STUSAB' : \"\",'city_BASENAME' : \"\", 'city_NAME' : \"\",  'status' : \"\" }\n",
    "\n",
    "\n",
    "def get_coordinates_aws(city, state):    \n",
    "    # THis costs money...\n",
    "    try:\n",
    "        result = client.search_place_index_for_text(FilterCountries=['USA'], IndexName='City_State_lookup', Text=f'{city}, {state}', MaxResults=3)\n",
    "        # print(f'The type of this variable is : {type(result)}')\n",
    "        lat = result['Results'][0]['Place']['Geometry']['Point'][0]\n",
    "        long = result['Results'][0]['Place']['Geometry']['Point'][1]\n",
    "        return True, str(lat), str(long), result\n",
    "    except:\n",
    "        return False, \"\", \"\", {}\n",
    "\n",
    "\n",
    "def get_coordinates_Nominatim(city, state):    \n",
    "    # Nominatim requests that we only make 1 request per second, otherwise we might get blocked.\n",
    "    try:\n",
    "        # location = geolocator.geocode({'city':city, 'state':state})\n",
    "        location = geolocator.geocode(f'{city} {state}', exactly_one=False, limit=3, addressdetails=True )\n",
    "        # print(location)\n",
    "        time.sleep(1)\n",
    "        # return True, str(location.latitude), str(location.longitude), location      \n",
    "        return True, str(location.longitude), str(location.latitude), location      \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(1)\n",
    "        return False, \"\", \"\", {}\n",
    "\n",
    "def get_county_information_from_census(lat, long, city, state):\n",
    "    # print (f'Lat : {lat }, Long : {long}')\n",
    "    url = f'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={lat}&y={long}&benchmark=4&vintage=4&format=json'\n",
    "    form_url = f'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={lat}&y={long}&benchmark=4&vintage=4'\n",
    "    # print(form_url)\n",
    "\n",
    "# Probably should be its own class at this point.\n",
    "    try:\n",
    "        response = requests.get(url)        \n",
    "        if(response.status_code != 200):\n",
    "            raise        \n",
    "        data = response.json()\n",
    "\n",
    "        county_GEOID = data['result']['geographies']['Counties'][0]['GEOID']    \n",
    "        county_fip = data['result']['geographies']['Counties'][0]['COUNTY']\n",
    "        county_BASENAME = data['result']['geographies']['Counties'][0]['BASENAME']\n",
    "\n",
    "        state_fip = data['result']['geographies']['States'][0]['STATE']\n",
    "        state_BASENAME = data['result']['geographies']['States'][0]['BASENAME']\n",
    "        state_STUSAB = data['result']['geographies']['States'][0]['STUSAB'] \n",
    "\n",
    "        city_BASENAME = data['result']['geographies']['County Subdivisions'][0]['BASENAME'] \n",
    "        city_NAME = data['result']['geographies']['County Subdivisions'][0]['NAME']  \n",
    "\n",
    "\n",
    "    except:\n",
    "        return null_response\n",
    "\n",
    "\n",
    "\n",
    "    print(f'{city_BASENAME} , {state_BASENAME} , {county_BASENAME} , {county_GEOID}')\n",
    "    census_object = {\n",
    "        'original_state' : state,\n",
    "        'original_city' : city,\n",
    "        'county_GEOID' : county_GEOID ,\n",
    "        'county_fip' :county_fip, \n",
    "        'county_BASENAME' :county_BASENAME ,\n",
    "        'state_fip' :state_fip ,\n",
    "        'state_BASENAME' : state_BASENAME ,\n",
    "        'state_STUSAB' : state_STUSAB ,\n",
    "        'city_BASENAME' : city_BASENAME, \n",
    "        'city_NAME' : city_NAME , \n",
    "        'form_url' : form_url,        \n",
    "        'status' : state_STUSAB == state } \n",
    "\n",
    "    return census_object\n",
    "\n",
    "\n",
    "\n",
    "def geocode_lat_long(lat, long, city=\"\", state=\"\"):\n",
    "    # print(\"  \")\n",
    "    # print(f'{city} , {state}')\n",
    "\n",
    "\n",
    "    if DBTools.get_row_count(\"aws_lookup_cache\", f\"city = '{city}' and state = '{state}'\") == 0: \n",
    "        print(f\"Geocoding {Lat}, {long}\")      \n",
    "        census_lookup_result = get_county_information_from_census(lat, long, city, state)\n",
    "        DBTools.insert_location_lookup_cache(city = city\n",
    "                                        , state = state\n",
    "                                        , geocode_response = {}\n",
    "                                        , census_lookup_result = census_lookup_result\n",
    "                                        , lat = lat\n",
    "                                        , long = long\n",
    "                                        )\n",
    "\n",
    "        return census_lookup_result\n",
    "\n",
    "    else:\n",
    "        print(f\"{city}, {state} already exists in the cache\")\n",
    "        return null_response\n",
    "\n",
    "\n",
    "def geocode_city_state(city, state, geocoder_service='aws'):\n",
    "    # print(\"  \")\n",
    "    # print(f'{city} , {state}')\n",
    "\n",
    "\n",
    "    if DBTools.get_row_count(\"aws_lookup_cache\", f\"city = '{city}' and state = '{state}'\") == 0: \n",
    "        print(f\"Geocoding {city}, {state}\")      \n",
    "        if(geocoder_service == 'aws'):\n",
    "            geocode_success, lat, long, geocode_response = get_coordinates_aws(city, state)\n",
    "        else:\n",
    "            geocode_success, lat, long, geocode_response = get_coordinates_Nominatim(city, state)\n",
    "\n",
    "        if (geocode_success == False):\n",
    "            return null_response\n",
    "        else:\n",
    "\n",
    "            census_lookup_result = get_county_information_from_census(lat, long, city, state)\n",
    "            DBTools.insert_location_lookup_cache(city = city\n",
    "                                            , state = state\n",
    "                                            , geocode_response = geocode_response\n",
    "                                            , census_lookup_result = census_lookup_result\n",
    "                                            , lat = lat\n",
    "                                            , long = long\n",
    "                                            )\n",
    "\n",
    "        return census_lookup_result\n",
    "\n",
    "    else:\n",
    "        print(f\"{city}, {state} already exists in the cache\")\n",
    "        return null_response\n",
    "\n",
    "        \n",
    "    # To see the results in an html form.\n",
    "    # https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x=-70.1441014&y=44.671539&benchmark=4&vintage=4\n",
    "    # https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x=-89.7134&y=45.6038&benchmark=4&vintage=4\n",
    "    \t\n",
    "\n",
    "# census_lookup_result = geocode_city_state('Parkridge','NJ', geocoder_service='aws')\n",
    "# using Nominatim geocoder - it doesn't do a good a job of fuzzy matching. There might be a way to get it to match better...but it's also free.\n",
    "# census_lookup_result = geocode_city_state('Parkridge','NJ', geocoder_service='Nominatim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing the geocoder, could write this into a unit test.\n",
    "test = False\n",
    "if test:\n",
    "    null_response = {'original_state' : \"\",'original_city' : \"\",'county_GEOID' : \"\",'county_fip' :\"\", 'county_BASENAME' :\"\",'state_fip' :\"\",'state_BASENAME' : \"\",'state_STUSAB' : \"\",'city_BASENAME' : \"\", 'city_NAME' : \"\",  'status' : \"\" }\n",
    "    test_list = [\n",
    "        {'city' : 'city', 'state' : 'state', \"geocode_response\" : {\"something\" : \"something\"} , \"census_lookup_result\" : null_response}, \n",
    "        {'city' : 'city1', 'state' : 'state1', \"geocode_response\" : {\"something\" : \"something\"} , \"census_lookup_result\" : null_response}, \n",
    "        {'city' : 'city2', 'state' : 'state2', \"geocode_response\" : {\"something\" : \"something\"} , \"census_lookup_result\" : null_response}, \n",
    "        {'city' : 'city3', 'state' : 'state3', \"geocode_response\" : {\"something\" : \"something\"} , \"census_lookup_result\" : null_response}]\n",
    "\n",
    "    for i in test_list:\n",
    "        city = i['city']\n",
    "        state = i['state']\n",
    "        geocode_response = i['geocode_response']\n",
    "        census_lookup_result = i['census_lookup_result']\n",
    "        DBTools.insert_location_lookup_cache(city=city, state=state, geocode_response=geocode_response, census_lookup_result=census_lookup_result, lat=\"123.456\", long=\"456.789\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the locations that we need from our data work\n",
    "pregrant_locations_file = r\"./data/pregrant/pregrant_locations.csv\"\n",
    "_pregrant_locations_df = pd.read_csv(pregrant_locations_file)\n",
    "\n",
    "# All the locations\n",
    "location_file = r\"./data/pregrant/location.tsv\"\n",
    "_location_df = pd.read_csv(location_file, sep='\\t')\n",
    "\n",
    "# Seriously, these leading zeros are annoying.\n",
    "_location_df[['state_fips', 'county_fips']] = _location_df[['state_fips','county_fips']].fillna(\"\")\n",
    "_location_df.loc[_location_df.state_fips!=\"\", 'state_fips'] = _location_df.loc[_location_df.state_fips!=\"\", 'state_fips'].astype(str).str.replace(\"\\.0\", \"\").str.zfill(2)\n",
    "_location_df.loc[_location_df.county_fips!=\"\", 'county_fips'] = _location_df.loc[_location_df.county_fips!=\"\", 'county_fips'].astype(str).str.replace(\"\\.0\", \"\").str.zfill(5)\n",
    "DBTools.insert_df(_location_df, \"pregrant_location_unfiltered\")\n",
    "\n",
    "# _location_df.query(\"county_fips != ''\").sort_values(by=['county_fips'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in the locations that are in the pregrant locations file that have missing data.\n",
    "reduced_location_df = pd.merge(_pregrant_locations_df, _location_df, left_on=['location_id'], right_on=['id'], how='left')\n",
    "print(reduced_location_df.shape)\n",
    "reduced_location_df = reduced_location_df.query(\"country == 'US' & county_fips == '' & city != '' & state != '' \", engine=\"python\")\n",
    "print(reduced_location_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_location_df.query(\"id=='baa6fcdc-cb8e-11eb-9615-121df0c29c1e'\")\n",
    "# _location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need all the data to join back once we have geocoded the locations.\n",
    "reinsert = False\n",
    "if reinsert:\n",
    "    pregrant_location_all_df = pd.merge(_pregrant_locations_df, _location_df, left_on=['location_id'], right_on=['id'], how='left')\n",
    "    DBTools.insert_df(pregrant_location_all_df, \"pregrant_location_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run the geocoder and persist the results.\n",
    "# We need to geta dataframe from the database for the final results.\n",
    "rerun = False\n",
    "if rerun:\n",
    "    reduced_location_df.apply(lambda x: geocode_city_state(x.city, x.state), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_crosswalk_df = DBTools.get_df(\"pregrant_location_crosswalk\", \"GEOID != ''\")\n",
    "location_crosswalk_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a file\n",
    "\n",
    "# I'm adding in a dummy row so the GEOIDS are string and not numeric. It won't match on anything else.\n",
    "location_crosswalk_df_dummy = location_crosswalk_df.head(1).copy()\n",
    "location_crosswalk_df_dummy['id'] = \"_\"\n",
    "location_crosswalk_df_dummy['GEOID'] = \"_\"\n",
    "location_crosswalk_df = pd.concat([location_crosswalk_df, location_crosswalk_df_dummy])\n",
    "\n",
    "\n",
    "location_crosswalk_df.to_csv(\"./data/pregrant/location_crosswalk.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a csv into dataframe\n",
    "reload = pd.read_csv(\"./data/pregrant/location_crosswalk.csv\", low_memory=False)\n",
    "reload.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## County Business Patterns \n",
    "### Pivoting tables so county is row and all predictors are columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fipstate</th>\n",
       "      <th>fipscty</th>\n",
       "      <th>naics</th>\n",
       "      <th>emp_nf</th>\n",
       "      <th>emp_county_naics</th>\n",
       "      <th>qp1_nf</th>\n",
       "      <th>qp1</th>\n",
       "      <th>ap_nf</th>\n",
       "      <th>ap</th>\n",
       "      <th>est</th>\n",
       "      <th>...</th>\n",
       "      <th>percent_of_us_emp</th>\n",
       "      <th>emp_us_naics</th>\n",
       "      <th>emp_us</th>\n",
       "      <th>location_quotient_county_state</th>\n",
       "      <th>location_quotient_county_us</th>\n",
       "      <th>year</th>\n",
       "      <th>censtate</th>\n",
       "      <th>cencty</th>\n",
       "      <th>empflag</th>\n",
       "      <th>n1_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>H</td>\n",
       "      <td>82</td>\n",
       "      <td>G</td>\n",
       "      <td>1075</td>\n",
       "      <td>G</td>\n",
       "      <td>4741</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>50710</td>\n",
       "      <td>134163349</td>\n",
       "      <td>3.226611</td>\n",
       "      <td>19.258521</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>J</td>\n",
       "      <td>10</td>\n",
       "      <td>J</td>\n",
       "      <td>108</td>\n",
       "      <td>J</td>\n",
       "      <td>491</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>109435</td>\n",
       "      <td>134163349</td>\n",
       "      <td>0.703160</td>\n",
       "      <td>1.088295</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>212</td>\n",
       "      <td>G</td>\n",
       "      <td>85</td>\n",
       "      <td>G</td>\n",
       "      <td>1208</td>\n",
       "      <td>G</td>\n",
       "      <td>4874</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>170784</td>\n",
       "      <td>134163349</td>\n",
       "      <td>2.814117</td>\n",
       "      <td>5.927539</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>H</td>\n",
       "      <td>165</td>\n",
       "      <td>H</td>\n",
       "      <td>5460</td>\n",
       "      <td>H</td>\n",
       "      <td>19533</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>637058</td>\n",
       "      <td>134163349</td>\n",
       "      <td>1.722931</td>\n",
       "      <td>3.084663</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td>H</td>\n",
       "      <td>159</td>\n",
       "      <td>H</td>\n",
       "      <td>1548</td>\n",
       "      <td>H</td>\n",
       "      <td>6311</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>1495473</td>\n",
       "      <td>134163349</td>\n",
       "      <td>1.168319</td>\n",
       "      <td>1.266255</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fipstate  fipscty naics emp_nf  emp_county_naics qp1_nf   qp1 ap_nf  \\\n",
       "2          1        1   113      H                82      G  1075     G   \n",
       "6          1        1   115      J                10      J   108     J   \n",
       "8          1        1   212      G                85      G  1208     G   \n",
       "13         1        1   221      H               165      H  5460     H   \n",
       "18         1        1   236      H               159      H  1548     H   \n",
       "\n",
       "       ap  est  ... percent_of_us_emp emp_us_naics     emp_us  \\\n",
       "2    4741    7  ...          0.000378        50710  134163349   \n",
       "6     491    3  ...          0.000816       109435  134163349   \n",
       "8    4874    5  ...          0.001273       170784  134163349   \n",
       "13  19533    8  ...          0.004748       637058  134163349   \n",
       "18   6311   20  ...          0.011147      1495473  134163349   \n",
       "\n",
       "   location_quotient_county_state location_quotient_county_us  year censtate  \\\n",
       "2                        3.226611                   19.258521  2020      NaN   \n",
       "6                        0.703160                    1.088295  2020      NaN   \n",
       "8                        2.814117                    5.927539  2020      NaN   \n",
       "13                       1.722931                    3.084663  2020      NaN   \n",
       "18                       1.168319                    1.266255  2020      NaN   \n",
       "\n",
       "   cencty empflag n1_4  \n",
       "2     NaN     nan  NaN  \n",
       "6     NaN     nan  NaN  \n",
       "8     NaN     nan  NaN  \n",
       "13    NaN     nan  NaN  \n",
       "18    NaN     nan  NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the data.\n",
    "naics_level = 2\n",
    "_cbp_lq_digit_naics_all_years_df = FileTools.load_df_from_parquet(f'cbp_lq_3digit_naics_all_years.gzip')                                                                   \n",
    "_cbp_lq_digit_naics_all_years_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cbp = pd.DataFrame()\n",
    "for i in range(2010,2021): \n",
    "# for i in range(2020,2021): \n",
    "    year = str(i)\n",
    "    print(f'Working on year : {year}')    \n",
    "\n",
    "    # Filter by the year we want.\n",
    "    temp_cbp_lq_digit_naics_all_years_df = _cbp_lq_digit_naics_all_years_df[\n",
    "        (_cbp_lq_digit_naics_all_years_df['year'] == year) \n",
    "        & (_cbp_lq_digit_naics_all_years_df['naics_level'] == naics_level)\n",
    "        ][[\n",
    "            'county_fips','qp1', 'ap', 'est','naics','location_quotient_county_state','location_quotient_county_us']].copy()\n",
    "\n",
    "\n",
    "    # Get some aggregations by county\n",
    "    group_cbp_lq_digit_naics_all_years_df = temp_cbp_lq_digit_naics_all_years_df[['county_fips','qp1', 'ap', 'est']].groupby(['county_fips']).sum().reset_index()\n",
    "    \n",
    "    \n",
    "    # Pivot all the naics codes up into columns\n",
    "    pivot_cbp_lq_2digit_naics_all_years_df = pd.pivot_table(temp_cbp_lq_digit_naics_all_years_df, values='location_quotient_county_us', index=['county_fips'],\n",
    "                        columns=['naics'], aggfunc=np.average, fill_value=0)\n",
    "\n",
    "\n",
    "    # Add the aggregations to the pivot table\n",
    "    merge_cbp_lq_2digit_naics_all_years_df = pivot_cbp_lq_2digit_naics_all_years_df.merge(group_cbp_lq_digit_naics_all_years_df, on='county_fips', how='left')\n",
    "    merge_cbp_lq_2digit_naics_all_years_df['year'] = year\n",
    "\n",
    "    # Do I need to do this?\n",
    "    merge_cbp_lq_2digit_naics_all_years_df.reset_index(inplace=True)\n",
    "    all_data_cbp = pd.concat([all_data_cbp, merge_cbp_lq_2digit_naics_all_years_df])\n",
    "\n",
    "FileTools.save_df_as_parquet(all_data_cbp, f'cbp_lq_{naics_level}digit_naics_all_years_PIVOT.gzip')\n",
    "FileTools.save_df_as_csv(all_data_cbp, f'cbp_lq_{naics_level}digit_naics_all_years_PIVOT.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_cbp_lq_digit_naics_all_years_df = _cbp_lq_digit_naics_all_years_df[\n",
    "        (_cbp_lq_digit_naics_all_years_df['year'] == 2020) \n",
    "        & (_cbp_lq_digit_naics_all_years_df['naics_level'] == 3)\n",
    "        ][[\n",
    "            'county_fips','qp1', 'ap', 'est','naics','location_quotient_county_state','location_quotient_county_us']].copy()\n",
    "\n",
    "temp_cbp_lq_digit_naics_all_years_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cbp = FileTools.load_df_from_parquet(f'cbp_lq_{naics_level}digit_naics_all_years_PIVOT.gzip')\n",
    "print(all_data_cbp.shape)\n",
    "all_data_cbp.head()\n",
    "# DBTools.truncate_and_insert_df(all_data_cbp, \"cbp_PIVOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Economic Indicators\n",
    "\n",
    "* CAINC4__ALL_AREAS_1969_2020.csv\n",
    "* https://virginia.box.com/s/v70g9niz9att0381tapsnmtffksq7ahw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data, there are some encoding issues so we need to explicitly specify the encoding.\n",
    "_CAINC4__ALL_AREAS_1969_2020_df = pd.read_csv(FileTools.get_full_file_path('CAINC4__ALL_AREAS_1969_2020.csv'),low_memory=False, encoding = \"ISO-8859-1\" )\n",
    "\n",
    "# Clean up the description field a bit.\n",
    "_CAINC4__ALL_AREAS_1969_2020_df['Description'] = _CAINC4__ALL_AREAS_1969_2020_df.Description.str.strip().str.replace(\"/\",\"\")\n",
    "\n",
    "# Seems like there is double data for 'Employer contributions for government social insurance' but they have different LineCodes so we can take out one of them.\n",
    "# The other descriptions are in 30's and on so we will take out 62.\n",
    "_CAINC4__ALL_AREAS_1969_2020_df = _CAINC4__ALL_AREAS_1969_2020_df[~(_CAINC4__ALL_AREAS_1969_2020_df.LineCode == 62)]\n",
    "# DBTools.truncate_and_insert_df(_CAINC4__ALL_AREAS_1969_2020_df, \"CAINC4__ALL_AREAS_1969_2020_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot the table so that county is row and all predictors are columns by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_ei = pd.DataFrame()\n",
    "for i in range(1969,2021):  \n",
    "# for i in range(1969,1970):\n",
    "    year = str(i)\n",
    "    print(f'Working on year : {year}')\n",
    "    temp_CAINC4__ALL_AREAS_1969_2020_df = _CAINC4__ALL_AREAS_1969_2020_df[[year, 'GeoFIPS',  'Description']].copy()\n",
    "    pivot_CAINC4__ALL_AREAS_1969_2020_df = pd.pivot_table(temp_CAINC4__ALL_AREAS_1969_2020_df, \n",
    "                                                    values=year\n",
    "                                                    ,index=['GeoFIPS']\n",
    "                                                    ,columns=['Description']\n",
    "                                                    ,aggfunc=np.sum, fill_value=0)\n",
    "\n",
    "    pivot_CAINC4__ALL_AREAS_1969_2020_df['year'] = year\n",
    "    pivot_CAINC4__ALL_AREAS_1969_2020_df.reset_index(inplace=True)\n",
    "    all_data_ei = pd.concat([all_data_ei, pivot_CAINC4__ALL_AREAS_1969_2020_df])   \n",
    "\n",
    "all_data_ei.GeoFIPS = all_data_ei.GeoFIPS.str.replace('\"','')\n",
    "\n",
    "FileTools.save_df_as_parquet(all_data_ei, 'CAINC4_ALL_AREAS_1969_2020_PIVOT.gzip')\n",
    "FileTools.save_df_as_csv(all_data_ei, 'CAINC4_ALL_AREAS_1969_2020_PIVOT.csv')\n",
    "# DBTools.truncate_and_insert_df(all_data_ei, \"CAINC4__ALL_AREAS_1969_2020_PIVOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_ei.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = FileTools.load_df_from_csv('CAINC4_ALL_AREAS_1969_2020_PIVOT.csv')\n",
    "test = pd.read_csv(FileTools.get_full_file_path('CAINC4_ALL_AREAS_1969_2020_PIVOT.csv'),low_memory=False, encoding = \"ISO-8859-1\" )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('UVACapstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1320d6b9c24963eeab91ed3c1b469bbfc0dc38b81cdaee0abc39ad090f12f690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
